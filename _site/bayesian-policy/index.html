<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.13.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Learning Probability Distributions in Bounded Action Spaces - Brandon Da Silva</title>
<meta name="description" content="Reinforcement Learning, Neural Networks, Bayesian">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Brandon Da Silva">
<meta property="og:title" content="Learning Probability Distributions in Bounded Action Spaces">
<meta property="og:url" content="http://localhost:4000/bayesian-policy/">


  <meta property="og:description" content="Reinforcement Learning, Neural Networks, Bayesian">



  <meta property="og:image" content="http://localhost:4000/images/ML-banner.jpg">





  <meta property="article:published_time" content="2018-11-12T00:00:00-05:00">





  

  


<link rel="canonical" href="http://localhost:4000/bayesian-policy/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Brandon Da Silva",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Brandon Da Silva Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Brandon Da Silva</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/machine-learning/" >Machine Learning</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  











<div class="page__hero"
  style=" "
>
  
    <img src="/images/ML-banner.jpg" alt="Learning Probability Distributions in Bounded Action Spaces" class="page__hero-image">
  
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/images/bio-photo.jpg" alt="Brandon Da Silva" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Brandon Da Silva</h3>
    
    
      <p class="author__bio" itemprop="description">
        Machine Learning Blog
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Toronto, ON</span>
        </li>
      

      
        
          
        
          
        
          
        
          
        
          
        
          
        
      

      

      
        <li>
          <a href="mailto:brandasilva9@gmail.com">
            <meta itemprop="email" content="brandasilva9@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/brandon-da-silva-a67496a7" itemprop="sameAs">
            <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/brandinho" itemprop="sameAs">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Learning Probability Distributions in Bounded Action Spaces">
    <meta itemprop="description" content="Reinforcement Learning, Neural Networks, Bayesian">
    <meta itemprop="datePublished" content="November 12, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Learning Probability Distributions in Bounded Action Spaces
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="overview">Overview</h2>

<p>In this post we will learn how to apply reinforcement learning in a probabilistic manner. More specifically, we will be looking at some of the difficulties in applying conventional approaches to bounded action spaces, and provide a solution. This blog assumes you have knowledge in deep learning. If not, check out <a href="http://neuralnetworksanddeeplearning.com/chap1.html">Michael Nielsen’s book</a> - it is very comprehensive and easy to understand.</p>

<h2 id="reinforcement-learning-background">Reinforcement Learning Background</h2>

<p>I am not going to provide a complete background on Reinforcement Learning (RL) because there are already some excellent resources online such as <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">Arthur Juliani’s blogs</a> and <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-">David Silver’s lectures</a>. I highly recommend going through both to get a solid understanding of the fundamentals. With that said, I will explain some concepts that are important for this blog post.</p>

<p>At the most basic level, the goal of RL is to learn a mapping from states to actions. To understand what this means, I think it is important to take a step back and understand the RL framework more generally. Cue the overused RL diagram:</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/AgentEnvironment.jpg" alt="alt" /></p>

<p>The first thing to notice is that there is a feedback loop between the agent and the environment. For clarity, the agent refers to the AI that we are creating, while the environment refers to the world that the agent has to navigate through. In order to navigate through an environment, the agent has to take actions. The specific actions will depend on the domain - we will describe a few fairly soon. After the agent takes an action, it receives an observation of the environment (the current state) and a reward (assuming we don’t have sparse rewards).</p>

<p>After interacting with the environment for long enough, we hope that our agent learns how to take actions that maximize its cumulative reward over the long-term. It is important to realize that the best action in one state is not necessarily the best action in another state. So going back to our statement about mapping states to actions, this simply means that we want our agent to learn the best actions to take in each environment state. The function that maps states to actions is called a policy and is denoted as <script type="math/tex">\pi(a \mid s)</script>. Usually we read <script type="math/tex">\pi(a \mid s)</script> as: <em>probability of taking action <script type="math/tex">a</script>, given we are in state <script type="math/tex">s</script></em>. However, just as a side note, your policy does not have to be defined probabilistically - you can define it deterministically as well.</p>

<p>Now let’s talk a bit about actions an agent can take. The first distinction I would like to make is between discrete actions and continuous actions. When we refer to discrete actions, we simply mean that there is a finite set of possible actions an agent can take. For example, in pong an agent can decide to move up or down. On the other hand, continuous actions have an infinite number of possibilities. An example of a continuous action, although kind of silly, is the hiding position of an agent if it is playing hide and seek.</p>

<p>Given enough time, the agent can theoretically hide anywhere - so the action space is unbounded. In contrast, we can have a continuous action space that is bounded. An example close to my heart is position sizing when trading a financial asset. The bounds are -1 (100% Short) and 1 (100% Long). To map states to that bounded action space, we can use <script type="math/tex">\tanh</script> in the final layer of a neural network. That seems pretty easy… so why am I writing a blog post about it? Often times we need more than just a deterministic output, especially when the underlying data has a low signal-to-noise ratio. The additional piece of information that we need is <em>uncertainty</em> in our agent’s decision. We will use a Bayesian approach to model a posterior distribution and sample from this distribution to estimate the uncertainty. Don’t worry if that doesn’t completely make sense yet - it will by the end of this post!</p>

<h2 id="probability-distributions">Probability Distributions</h2>

<p>For a great introduction to Bayesian statistics I suggest reading <a href="https://www.countbayesie.com">Will Kurt’s blog</a> - Count Bayesie. It’s awesome.</p>

<p>Distributions can be thought of as representing beliefs about the world. Specifically as it relates to our task at hand, the probability distributions represent our beliefs in how good an action is, given the state. In the financial markets context, where the action space is continuous and bounded between -1 and 1, a mean close to 1 represents a belief that it is a good time to buy that asset, so we should long it. A mean close to -1 represents the opposite, so we should short the asset. Building on this example, if the standard deviation of our distribution is large (small) then our agent is uncertain (certain) in its decision. In other words, if the agent’s policy has a large standard deviation, then it has not developed a strong belief yet.</p>

<p>Whenever you hear anyone talking about Bayesian statistics, you always hear the terms “prior” and “posterior”. Simply put, a prior is your belief about the world <em>before</em> receiving new information. However, once you receive new information, then you update your prior distribution to form a posterior distribution. After that, if you receive more information, then your posterior becomes your prior, and the new information gets incorporated to form a new posterior distribution. Essentially, there is this feedback loop of continual learning that happens as more and more new information gets processed by your agent. Below we visually show one iteration of this loop:</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/prior_posterior.png" alt="alt" /></p>

<p>Our goal is to learn a good posterior distribution on actions, conditioned on the state that the agent is in. If you are familiar with <a href="https://arxiv.org/pdf/1506.02142.pdf">this paper</a>, then you might be thinking that we can just use Monte Carlo (MC) dropout with a <script type="math/tex">\tanh</script> output layer. For those who are not familiar with this concept, let me explain. Dropout is a technique that was originally used for neural network regularization. With each pass, it will randomly “drop” neurons from each hidden layer by setting their output to 0. This reduces the output’s dependency on any one particular neuron, which should help generalization. However, researchers at Cambridge found that using dropout during inference can be used to approximate a posterior distribution. This is because each time you pass inputs through the network, a different set of neurons will be dropped, so the output is going to be different for each run - creating a distribution of outputs.</p>

<p>The great thing about this architecture is that you can easily pass gradients through the policy network. The loss function that we are minimizing throughout this blog is <script type="math/tex">\mathcal{L} = - r \times \pi(s)</script>, where <script type="math/tex">r</script> denotes the reward and <script type="math/tex">\pi(s)</script> denotes the policy output given the states (i.e. the action). We wanted to demonstrate how the distribution changes in a controlled environment. So we use the same state input throughout all our experiments and continually feed it a positive reward to view the changes during training. Below is the first example using the MC Dropout method and a <script type="math/tex">\tanh</script> output layer.</p>

<p><img src="/images/MC_dropout_posterior.gif" alt="Alt Text" /></p>

<p>I omitted a kernel density estimation (KDE) plot on top of the histogram because as training progressed, the KDE became much more jagged and not representative of the actual probability density function (PDF). I was using <code class="highlighter-rouge">sns.kdeplot</code>, if anyone knows how to fix this, please let me know in the comments section!</p>

<p>There are two things that I don’t particularly like about this approach. The first is that it is possible to have multiple peaks in the distribution, as seen when the neural network is first initialized. I realize that as training went on, only one peak emerged. However, the fact that an agent can potentially learn such a distribution (with multiple peaks) makes me uncomfortable. If we go back to our example in the financial markets, an action of -1 will have the exact opposite reward compared to an action of 1 (because it is the other side of the trade), so having peaks at both ends of the spectrum is quite confusing. I would much rather just have one peak near 0 with a large standard deviation if the agent is uncertain which action to take. The second is that it becomes overly optimistic in its decision when compared to a gaussian output (we will see this later), which could possibly indicate that it is understating the uncertainty.</p>

<p>I will digress for a moment to state that a multimodal distribution (a distribution with multiple peaks) is not always bad. For example if you imagine an agent trying to navigate through a room and their policy dictates the angle at which they will move, then there could be two different angles that, while momentarily will send them in different directions, will ultimately lead them to the same end location. However, for this post, we will stick to the example in the financial markets, where a multimodal distribution doesn’t make sense.</p>

<p>Instead of using MC dropout, we can try using a normal distribution in the output and see if things improve. The architecture of our neural network now becomes:</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/gaussian_output.png" alt="alt" /></p>

<p>If our neural network parameters are denoted by <script type="math/tex">\theta</script>, then we can define <script type="math/tex">\mu_{\theta}</script> and <script type="math/tex">\sigma_{\theta}</script> as outputs of the neural network, such that:</p>

<script type="math/tex; mode=display">\pi \sim \mathcal{N}(\mu_{\theta}(s), \sigma_{\theta}(s))</script>

<h2 id="reparameterization-trick">Reparameterization Trick</h2>

<p>We want to update the policy network with backpropagation (similar to what we did with the MC dropout architecture), but you’ll notice that we have a bit of a problem - a random variable is now part of the computation graph. This is a problem because backpropagation cannot flow through a random node. However, by using the reparameterization trick, we can move the random node outside of the computation graph and then feed in samples drawn from the distribution as constants. Inference is the exact same, but now our neural network can perform backpropagation.</p>

<p>To do this, we define a random variable <script type="math/tex">\varepsilon</script>, which does not depend on <script type="math/tex">\theta</script>. The new architecture becomes:</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/gaussian_reparameterized.png" alt="alt" /></p>

<script type="math/tex; mode=display">\varepsilon \sim \mathcal{N}(0,I)</script>

<script type="math/tex; mode=display">\pi = \mu_{\theta}(s) + \sigma_{\theta}(s) \cdot \varepsilon</script>

<p>Python code to take the random variable outside of the computation graph is shown below (I’m only showing the relevant portion of the computation graph):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

  <span class="n">policy_mu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">previous_layer</span><span class="p">,</span> <span class="n">weights_mu</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_mu</span><span class="p">)</span>
  <span class="n">policy_sigma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">previous_layer</span><span class="p">,</span> <span class="n">weights_sigma</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_sigma</span><span class="p">)</span>                

  <span class="n">epsilon</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">policy_sigma</span><span class="p">),</span> <span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">stddev</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

  <span class="n">policy</span> <span class="o">=</span> <span class="n">policy_mu</span> <span class="o">+</span> <span class="n">policy_sigma</span> <span class="o">*</span> <span class="n">epsilon</span>
</code></pre></div></div>

<p>Now to get the neural network to work in a bounded space, we can clip outputs to be between -1 and 1. We simply change the last line of code in our network to:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">policy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">policy_mu</span> <span class="o">+</span> <span class="n">policy_sigma</span> <span class="o">*</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">clip_value_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">clip_value_max</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>The resulting distribution is shown below:</p>

<p><img src="/images/clipped_posterior.gif" alt="Alt Text" /></p>

<p>There is one obvious flaw in this approach - all of the clipped values get a value of either -1 or 1, which creates a very unbalanced distribution. To fix this, we will sample <script type="math/tex">\varepsilon</script> from a truncated normal distribution.</p>

<h2 id="truncated-normal-solution">Truncated Normal Solution</h2>

<p>A truncated normal distribution is similar to a normal distribution, in that it is defined by a mean (<script type="math/tex">\mu</script>) and standard deviation (<script type="math/tex">\sigma</script>). However, the key distinction is that the distribution’s range is limited to be within a lower and upper bound. Typically the lower bound is denoted by <script type="math/tex">a</script> and the upper bound is denoted by <script type="math/tex">b</script>, but I’m going to use <script type="math/tex">L</script> and <script type="math/tex">U</script> because I think it is easier to follow.</p>

<p>One might think that the bounds we define for the distribution should be the same as the bounds of our policy, but that won’t work if we want to use reparameterization. This is because the bounds apply to <script type="math/tex">\varepsilon</script> and not <script type="math/tex">\pi</script>. Since we expand <script type="math/tex">\varepsilon</script> by <script type="math/tex">\sigma</script> and shift it by <script type="math/tex">\mu</script>, then applying bounds of -1 and 1 will result in a <script type="math/tex">\pi</script> that extends beyond the bounds. To make this point more clear, let’s say we defined our bounds <script type="math/tex">-1 \leq \varepsilon \leq 1</script>, and <script type="math/tex">\mu = 0.5 , \, \sigma = 1</script>. If we generate a sample <script type="math/tex">\varepsilon = 0.9</script>, then after you apply the transformation <script type="math/tex">\mu + \sigma \cdot \varepsilon</script>, you get <script type="math/tex">\pi = 0.5 + 1 \cdot 0.9 = 1.4</script>, which is beyond the upper bound.</p>

<p>To generate the proper upper and lower bounds, we will use the equations below:</p>

<script type="math/tex; mode=display">L = \frac{-1 - \mu_{\theta}}{\sigma_{\theta}}</script>

<script type="math/tex; mode=display">U = \frac{1 - \mu_{\theta}}{\sigma_{\theta}}</script>

<p>Using our previous example, we find that <script type="math/tex">U = 0.5</script>, which means that the largest <script type="math/tex">\varepsilon</script> we can sample is 0.5. Plugging this into our reparameterized equation, we see that the largest <script type="math/tex">\pi</script> we can generate is 1. Similarly, <script type="math/tex">L = -1.5</script>, which means that the lowest <script type="math/tex">\pi</script> we can generate is -1. Perfect, we figured it out!</p>

<p>Given the PDF for a normal distribution:</p>

<script type="math/tex; mode=display">p(\varepsilon) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{\varepsilon - \mu}{\sigma}\right)^2}</script>

<p>We will let <script type="math/tex">F(\varepsilon)</script> denote our cumulative distribution function (CDF). Our truncated density now becomes:</p>

<script type="math/tex; mode=display">p(\varepsilon \mid L \leq \varepsilon \leq U) = \frac{p(\varepsilon)}{F(U) - F(L)} \, \, \text{for} \, L \leq \varepsilon \leq U</script>

<p>The denominator, <script type="math/tex">F(U) - F(L)</script>, is the normalizing constant that allows the truncated density to integrate to 1. The reason we do this is because, as shown below, we are only sampling from a portion of <script type="math/tex">p(\varepsilon)</script>.</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/truncated_distribution.png" alt="alt" /></p>

<p>You can import <code class="highlighter-rouge">scipy</code> and use the following function to generate samples from a truncated normal distribution:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>

  <span class="n">mu_dims</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># Dimensionality of the Mu generated by the Neural Network
</span>
  <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>
  <span class="n">sn_mu</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Standard Normal Mu
</span>  <span class="n">sn_sigma</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Standard Normal Sigma
</span>
  <span class="n">generator</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">truncnorm</span><span class="p">((</span><span class="n">lower_bound</span> <span class="o">-</span> <span class="n">sn_mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">sn_sigma</span><span class="p">,</span> <span class="p">(</span><span class="n">upper_bound</span> <span class="o">-</span> <span class="n">sn_mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">sn_sigma</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">sn_mu</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">sn_sigma</span><span class="p">)</span>
  <span class="n">epsilons</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">rvs</span><span class="p">([</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">mu_dims</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/images/posterior.gif" alt="Alt Text" /></p>

<p>This distribution looks a lot nicer than both of the previous approaches, and has some nice properties:</p>
<ul>
  <li>It only has one peak at all times</li>
  <li>Outputs do not need to be clipped</li>
  <li>The policy doesn’t look overly optimistic.</li>
</ul>

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>In this post, we examined a few approaches to approximating a posterior distribution over our policy. Ultimately, we feel that using a neural network with a truncated normal policy is the best approach out of those examined. We learned how to reparameterize a truncated normal, which allows us to train the policy network using backpropagation.</p>

<h2 id="acknowledgments">Acknowledgments</h2>

<p>I would like to thank <a href="https://www.linkedin.com/in/alek-riley-609073110/">Alek Riley</a> for his feedback on how to improve the clarity of certain explanations.</p>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = http://localhost:4000;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = /bayesian-policy; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://brandinho-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#bayesian" class="page__taxonomy-item" rel="tag">Bayesian</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#reinforcement-learning" class="page__taxonomy-item" rel="tag">Reinforcement Learning</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#reparameterization" class="page__taxonomy-item" rel="tag">Reparameterization</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-11-12T00:00:00-05:00">November 12, 2018</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Learning+Probability+Distributions+in+Bounded+Action+Spaces%20http%3A%2F%2Flocalhost%3A4000%2Fbayesian-policy%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fbayesian-policy%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http%3A%2F%2Flocalhost%3A4000%2Fbayesian-policy%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fab fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fbayesian-policy%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="#" class="pagination--pager disabled">Previous</a>
    
    
      <a href="/genetic-algorithm/" class="pagination--pager" title="Learning How to Run with Genetic Algorithms
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/mario-ppo/" rel="permalink">Playing Super Mario Bros with Proximal Policy Optimization
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  16 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Reinforcement Learning, Neural Networks, Policy Gradient
</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/genetic-algorithm/" rel="permalink">Learning How to Run with Genetic Algorithms
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  9 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Reinforcement Learning, Neural Networks, Genetic Algorithm
</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Brandon Da Silva. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>








<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  </body>
</html>
