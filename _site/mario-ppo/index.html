<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.13.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Playing Super Mario Bros with Proximal Policy Optimization - Brandon Da Silva</title>
<meta name="description" content="Reinforcement Learning, Neural Networks, Policy Gradient">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Brandon Da Silva">
<meta property="og:title" content="Playing Super Mario Bros with Proximal Policy Optimization">
<meta property="og:url" content="http://localhost:4000/mario-ppo/">


  <meta property="og:description" content="Reinforcement Learning, Neural Networks, Policy Gradient">



  <meta property="og:image" content="http://localhost:4000/images/mario-banner.png">





  <meta property="article:published_time" content="2018-12-02T00:00:00-05:00">





  

  


<link rel="canonical" href="http://localhost:4000/mario-ppo/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Brandon Da Silva",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Brandon Da Silva Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Brandon Da Silva</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/machine-learning/" >Machine Learning</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  











<div class="page__hero"
  style=" "
>
  
    <img src="/images/mario-banner.png" alt="Playing Super Mario Bros with Proximal Policy Optimization" class="page__hero-image">
  
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/images/bio-photo.jpg" alt="Brandon Da Silva" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Brandon Da Silva</h3>
    
    
      <p class="author__bio" itemprop="description">
        Machine Learning Blog
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Toronto, ON</span>
        </li>
      

      
        
          
        
          
        
          
        
          
        
          
        
          
        
      

      

      
        <li>
          <a href="mailto:brandasilva9@gmail.com">
            <meta itemprop="email" content="brandasilva9@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/brandon-da-silva-a67496a7" itemprop="sameAs">
            <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/brandinho" itemprop="sameAs">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Playing Super Mario Bros with Proximal Policy Optimization">
    <meta itemprop="description" content="Reinforcement Learning, Neural Networks, Policy Gradient">
    <meta itemprop="datePublished" content="December 02, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Playing Super Mario Bros with Proximal Policy Optimization
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  17 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="overview">Overview</h2>

<p>In this post, our AI agent will learn how to play Super Mario Bros by using Proximal Policy Optimization (PPO). We want our agent to learn how to play by only observing the raw game pixels so we use convolutional layers early in the network, followed by dense layers to get our policy and state-value output. The architecture of our model is shown below.</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/mario-model-architecture.png" alt="alt" /></p>

<p>Code is not yet available, but I will post it on github sometime this week.</p>

<p>Throughout this post, I’m going to explain each of the model’s components. First we start with the convolutional layers.</p>

<h2 id="convolutional-neural-network">Convolutional Neural Network</h2>

<p>Convolutional neural networks (CNNs) are widely used in image recognition, and have achieved very impressive results to date. They have their own set of issues, such as the inability to take important spatial hierarchies into account, which <a href="https://arxiv.org/pdf/1710.09829.pdf">capsule networks</a> attempt to address. However, we don’t think that this significantly impacts an agent’s ability to play a video game from raw pixels so convolutional layers will be just fine for our algorithm.</p>

<p>Unfortunately I’m not going to fully explain CNNs because that would take a whole post on its own. Rather, I’m going to explain some of the most important concepts for our model. If you want a more detailed explanation, I highly recommend <a href="https://colah.github.io">Chrisopher Olah’s blog</a> - all his posts are incredible. Also <a href="https://www.coursera.org/learn/convolutional-neural-networks">Andrew Ng’s course</a> is awesome!</p>

<p>The first thing to understand is that every image is comprised of pixels, and every pixel is represented as a numerical value (or combination of values). The images from the game screen use the RGB color model, which means that for each pixel in the picture, there are going to be 3 numbers associated with it. The numbers correspond to how much red, green, and blue light to add to the image. An example of the RGB codes for the Mario picture are shown below:</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/mario-color-codes.png" alt="alt" /></p>

<p>Okay great, now what do we do with these pixel values? Convolutional layers are a great way to deal with raw pixel inputs into a neural network. Each convolutional layers consists of multiple filters, which extract important information about an image. You can think of each convolutional layer as a building block for the next. For example, the first layer can put together the pixels to form edges, the second layer can put together the edges to form shapes, the third layer can put the shapes together to form objects, etc.</p>

<p>The filters work by performing an operation called convolution, shown in the image below. The operation works by taking the sum of the element-wise product between a portion of the image and the filter (also called a kernel). It focuses on a portion of the image because we need the two matrices to be the same size. In our example, we perform convolution on the bottom-right portion of the image. The filter shown below is specifically designed to detect vertical edges in an image. However, in practice we don’t preset the filter weights to perform a specific task - instead the neural network will learn the weights that it deems the best with backpropagation.</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/convolution-operation.png" alt="alt" /></p>

<p>I know I said that the operation being performed above is convolution, but that is not completely true… We’re technically performing cross-correlation, but everyone refers to this operation in the neural network context as convolution. Let me explain why. To actually perform convolution, you need to either flip the source image or the kernel. The reason why we don’t do this for CNNs is because it adds unnecessary complexity. Why is it unnecessary? Because the neural network learns the weights for the kernel anyways, so if you needed to flip the kernel, the CNN will automatically learn the flipped kernel weights, making the actual flipping pointless. Since flipping does not make a difference, cross-correlation is equivalent to convolution in this context.</p>

<p>As mentioned before, the kernel is applied to a portion of the image, so we have to slide the kernel over the whole image to account for all the portions. Below we show an example of the filter in action! We used some different numbers - they don’t actually mean anything, I just made them up:</p>

<p style="text-align: center;"><img src="/images/ConvNet.gif" alt="Alt Text" /></p>

<p>The last concept that I want to introduce for CNNs is stride. The stride determines how many pixels the filter jumps over between convolution operations. For example, in our animation above, the stride was 1 because it moved one pixel at a time. But if we specify a stride of 2, then it will move two pixels at a time (skipping over one pixel). The larger the stride, the smaller the output from the convolutional layer. Below we show what a stride of 2 looks like for the same input and kernel:</p>

<p style="text-align: center;"><img src="/images/ConvNet2.gif" alt="Alt Text" /></p>

<p>Now that we understand how the neural network is able to deal with pixelated inputs, we will move onto the feed-forward (dense) portion of our model - it splits into a value estimation stream and a policy stream.</p>

<h2 id="the-value-function">The Value Function</h2>

<p>In reinforcement learning, we often care about value functions - specifically, the state-value function <script type="math/tex">V(s)</script> and the action-value function <script type="math/tex">Q(s,a)</script>. Before diving into some math, I want to explain these concepts intuitively. <script type="math/tex">V(s)</script> tells us how good it is to be in a particular state. In Super Mario Bros, the goal is to go all the way to the right side of the map, as fast as possible. Thus, we get a positive (negative) reward if we move to the right (left), while getting a negative reward every time the clock ticks. Let’s let M1 and M2 represent two of Mario’s possible positions. If we define <script type="math/tex">V(s)</script> as the expectation of <script type="math/tex">G_t</script>, which is the cumulative discounted reward from time step <script type="math/tex">t</script>, then we realize that <script type="math/tex">V(s_{M2}) > V(s_{M1})</script>.</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/mario-value.png" alt="alt" style="max-width: 300px; height: auto;" /></p>

<p>If my previous statement did not completely make sense, let’s make it a bit more concrete with some math. Let’s let <script type="math/tex">R_t</script> represent the reward from time step <script type="math/tex">t</script>. We will define the cumulative discounted reward from time step <script type="math/tex">t</script> as:</p>

<script type="math/tex; mode=display">G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^kR_{t+k+1}</script>

<p>where <script type="math/tex">\gamma</script> is a discount factor that we apply to future rewards. This is a math trick that makes an infinite sum finite since <script type="math/tex">0 \leq \gamma \leq 1</script>. Although technically if <script type="math/tex">\gamma = 1</script> then the sum is still infinite because all future rewards have an equal weight. However, we generally use <script type="math/tex">% <![CDATA[
\gamma < 1 %]]></script>.</p>

<p>Now that we know how <script type="math/tex">G_t</script> is defined mathematically, let’s revisit our previous statement: <script type="math/tex">V(s_{M2}) > V(s_{M1})</script>. The farther Mario is from the right, the longer it takes to get to the end of the map. If it takes longer to get to the end of the map, then we have to add up more negative rewards to our cumulative sum (since we get a negative reward every time the clock ticks). Thus, it makes sense that <script type="math/tex">V(s_{M2}) > V(s_{M1})</script>.</p>

<p>Great, now that we have an intuition into how the state-value function works, let’s do some algebra to get a very important equation in reinforcement learning:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
	V(s) &= \mathbb{E}[\, G_t \, | \, S_t=s \,] \\
    &= \mathbb{E}[\, R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \, | \, S_t=s \,] \\
    &= \mathbb{E}[\, R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \ldots) \, | \, S_t=s \,] \\
    &= \mathbb{E}[\, R_{t+1} + \gamma G_{t+1} \, | \, S_t=s \,] \\
    &= \mathbb{E}[\, R_{t+1} + \gamma V(S_{t+1}) \, | \, S_t=s \,]
\end{align*} %]]></script>

<p>The equation that we end up with is know as the Bellman equation. If we think about it, it’s actually quite intuitive: the value for being in a particular state is equal to the expected reward we will receive from that state plus the discounted expected value of being in the next state. Let’s break this down a bit more. If the value for being in a state is equal to the sum of discounted future rewards, then <script type="math/tex">V(s_{t+1})</script> is the sum of discounted rewards after <script type="math/tex">t+1</script>. So if we add <script type="math/tex">R_{t+1}</script> to <script type="math/tex">\gamma V(s_{t+1})</script>, then we get the sum of discounted rewards after <script type="math/tex">t</script>, which is <script type="math/tex">V(s)</script>.</p>

<p>Alternatively, we can write the Bellman equation as,</p>

<script type="math/tex; mode=display">V(s)=\mathcal{R}_s + \gamma \sum_{s^\prime \in \mathcal{S}} \mathcal{P}_{ss^\prime} V(s^\prime)</script>

<p>where <script type="math/tex">\mathcal{P}_{ss^\prime}</script> refers to the probability transition matrix (i.e. the probability of moving from <script type="math/tex">s</script> to <script type="math/tex">s^{\prime}</script> for all <script type="math/tex">\mathcal{S}</script>).</p>

<p>Up until now, we were talking about the state-value function, but what about <script type="math/tex">Q(s,a)</script>? Most times, people actually care more about <script type="math/tex">Q</script> than <script type="math/tex">V</script>. The reason is because they want to know how to act in a given state, rather than the value of being in a state. This is exactly what <script type="math/tex">Q(s,a)</script> helps you determine because it tells you the value for taking a specific action in a given state. Thus, if you calculate the Q-value for all actions you can take (assuming the action space is discrete), then you can choose the action that has the maximum value. The super popular Q-learning algorithm learns the mapping from states to Q-values, so that an agent knows which actions will yield the highest cumulative discounted reward.</p>

<p>Let’s solidify our understanding of state-value and action-value. There is going to be a bit more math in this part, so get ready! First, let’s define a new term: the mapping from states to actions is defined as the policy and is denoted as <script type="math/tex">\pi(a \mid s)</script>. Although policies can be deterministic, we are going to read <script type="math/tex">\pi(a \mid s)</script> as “the probability of taking an action given the state”. I find that reading equations out loud in plain english helps solidify my understanding, so that’s what I’m going to do for the next few equations.</p>

<p>First we show the value of being in state <script type="math/tex">s</script> by following policy <script type="math/tex">\pi</script>. It is equal to the sum of Q-values, which correspond to particular actions, multiplied by the probability of taking those actions according to policy <script type="math/tex">\pi</script>.</p>

<script type="math/tex; mode=display">v_{\pi}(s)=\sum_{a \in \mathcal{A}}\pi(a \mid s)q_{\pi}(s,a)</script>

<p>Let’s break down <script type="math/tex">v_{\pi}(s)</script> in english:</p>

<ul>
  <li>In any state, there are multiple actions that we can take</li>
  <li>We take each action according to a probability distribution</li>
  <li>Each action has a different value associated with it</li>
  <li>Thus, the value of being in a state is equal to the weighted average of the action-values, in which the weights are the probabilities of taking each action</li>
</ul>

<p>Next we show the value of taking action <script type="math/tex">a</script> in state <script type="math/tex">s</script> by following policy <script type="math/tex">\pi</script>. It is equal to the expected reward from taking an action plus the discounted expected value of being in the next state.</p>

<script type="math/tex; mode=display">q_{\pi}(s,a)=\mathcal{R}_s^a + \gamma \sum_{s^\prime \in \mathcal{S}}\mathcal{P}_{ss^\prime}^{a}v_{\pi}(s^\prime)</script>

<p>Let’s break down <script type="math/tex">q_{\pi}(s,a)</script> in english:</p>

<ul>
  <li>For any action an agent takes, it receives a reward</li>
  <li>When an agent takes an action, it can end up in a different state
    <ul>
      <li>Image if your action was to move to the right - your agent is now in a new state</li>
    </ul>
  </li>
  <li>Sometimes environments have randomness embedded in them
    <ul>
      <li>Imagine if you try to move to the right, but wind pushes you back and you end up to the left of your original position</li>
    </ul>
  </li>
  <li>Thus, by taking an action in a given state, there is a probability that the agent will end up in various new states</li>
  <li>As a result, the value of taking an action in a given state is equal to the immediate reward from taking that action plus the weighted average of state-values for the next state multiplied by a discount factor.
    <ul>
      <li>The weights are the probabilities of ending up in the next states.</li>
    </ul>
  </li>
</ul>

<p>The previous two equations shown were half-step lookaheads. To show the full one-step lookaheads, we can plug in the previous equations to obtain the following:</p>

<script type="math/tex; mode=display">v_{\pi}(s)=\sum_{a \in \mathcal{A}}\pi(a \mid s)\left(\mathcal{R}_s^a + \gamma \sum_{s^\prime \in \mathcal{S}}\mathcal{P}_{ss^\prime}^{a}v_{\pi}(s^\prime)\right)</script>

<script type="math/tex; mode=display">q_{\pi}(s,a)=\mathcal{R}_s^a + \gamma \sum_{s^\prime \in \mathcal{S}}\mathcal{P}_{ss^\prime}^{a}\sum_{a^\prime \in \mathcal{A}}\pi(a^\prime|s^\prime)q_{\pi}(s^\prime,a^\prime)</script>

<p>If you understood the intuition for the first two equations, then you should have no problem with the two equations above - they are simply an extension using the exact same logic.</p>

<h2 id="policy-gradient">Policy Gradient</h2>

<p>What if we want to skip the middle part and just learn a mapping from states to actions without estimating the value of taking an action? We can do this with the policy gradient method, in which we explicitly learn <script type="math/tex">\pi</script>! Well sort of… we will soon see why we will actually need to incorporate the value function, but until then, let’s walk through a simple implementation of a policy gradient. Let’s consider the loss function:</p>

<script type="math/tex; mode=display">\mathcal{L} = r \times \log \pi(s,a)</script>

<p>We want to maximize <script type="math/tex">\mathcal{L}</script>, which is equivalent to minimizing <script type="math/tex">-\mathcal{L}</script> (we usually perform gradient descent, so minimizing a loss function is the convention). By minimizing <script type="math/tex">-\mathcal{L}</script>, we ensure that we increase the probability of taking an action that gives us a positive reward, and decrease the probability of taking an action that gives us a negative reward. That seems like a good idea, right? Not really… let’s go through an example to understand why. Imagine there are 3 actions that an agent can take with rewards of <script type="math/tex">[-1,3,20]</script> in particular state. There are two main problems with this approach:</p>

<ol>
  <li>Credit Assignment Problem</li>
  <li>Multiple “Good” Actions</li>
</ol>

<p>The credit assignment problem refers to the fact that rewards can be temporally delayed. For example, if an agent takes an action in time step <script type="math/tex">t</script>, the reward might come well after <script type="math/tex">t+1</script>. An example in Super Mario Bros is when our agent has to jump over a tube; multiple frames elapse from the time it presses the jump button to the time it actually makes it over the tube. The number of time steps that can possibly elapse between actions and rewards differ for each situation, so how do we solve this problem? Although this is not a perfect solution, we can use value functions, specifically <script type="math/tex">q_{\pi}(s,a)</script>. Since <script type="math/tex">q_{\pi}(s,a)</script> sums all future discounted rewards from taking action <script type="math/tex">a</script> and following policy <script type="math/tex">\pi</script>, our agent can take into account rewards that are temporally delayed. Our loss function now becomes:</p>

<script type="math/tex; mode=display">\mathcal{L} = q_{\pi}(s,a) \times \log \pi(s,a)</script>

<p>Let’s now assume that <script type="math/tex">[-1,3,20]</script> represents Q-values instead of rewards. We still have an issue because there are multiple actions that have a positive expected value. Imagine if we sample the second action, which has a positive Q-value. Based on our new policy gradient loss function, the parameter update would increase the probability of taking that action since <script type="math/tex">q_{\pi}(s,a)</script> is positive. But what about action 3? It had a much higher Q-value than action 2, so instead we need a way to tell the model to decease the probability of selecting action 2 and instead select action 3. That is what advantage helps us do.</p>

<h2 id="the-advantage-function">The Advantage Function</h2>

<p>Rather than looking at how good it is to take an action, advantage tells us how good an action is relative to other actions. This subtlety is important because we want to select actions that are better than average, as opposed to any action that has a positive expected value. To do this, we have to strip out the state-value from the action-value to get a pure estimate of how good an action is. We define advantage as:</p>

<script type="math/tex; mode=display">A(s,a) = Q(s,a) - V(s)</script>

<p>If we assume that our policy follows a uniform distribution (equal probability for each action), then <script type="math/tex">V(s) = 7.33</script>, which means that <script type="math/tex">A(s,a) = [-8.3,-4.3,12.7]</script>. Using our new loss function for policy gradients,</p>

<script type="math/tex; mode=display">\mathcal{L} = A_{\pi}(s,a) \times \log \pi(s,a)</script>

<p>we see that after selecting action 2, our agent will decrease the probability of selecting that action again in the same state because it has a negative advantage (its value is worse than the average). This is great, it does exactly what we want it to do! However, we don’t know the true advantage function (much like the value functions), so we have to estimate it. Luckily, there are a few ways to do this, but I’m going to focus on one method - using the temporal difference error (<script type="math/tex">\delta_{TD}</script>) from our value estimation.</p>

<p>Let me back up a little to explain what temporal difference error is. Remember when we saw this somewhat complicated equation earlier:</p>

<script type="math/tex; mode=display">q_{\pi}(s,a)=\mathcal{R}_s^a + \gamma \sum_{s^\prime \in \mathcal{S}}\mathcal{P}_{ss^\prime}^{a}\sum_{a^\prime \in \mathcal{A}}\pi(a^\prime|s^\prime)q_{\pi}(s^\prime,a^\prime)</script>

<p>Well it turns out that it will come in handy after all! Just a refresher - the equation above considers all possible paths. But what if we just sample one action from our policy and sample the next state from the environment? Well then it becomes:</p>

<script type="math/tex; mode=display">q_{\pi}(s,a) = r + \gamma v_{\pi}(s^{\prime})</script>

<p>Keep this in mind while I explain <script type="math/tex">\delta_{TD}</script>. As the name implies, temporal difference error refers to the difference between the one-step lookahead and the current estimate. We can calculate <script type="math/tex">\delta_{TD}</script> for either the state-value or action-value, but in this example we’re using the state-value. When we sample, the one-step lookahead equation for state-value becomes <script type="math/tex">v_{\pi}(s) = r + \gamma v_{\pi}(s^{\prime})</script>. You’ll notice that the left side is a pure estimate, while the right side is a mix of estimation and actual data from the environment. This means that the right side contains more information about the environment than the left! By taking the difference between the two we obtain:</p>

<script type="math/tex; mode=display">\delta_{TD} = r + \gamma v_{\pi}(s^{\prime}) - v_{\pi}(s)</script>

<p>and by minimizing <script type="math/tex">\delta_{TD}^2</script>, we move our value estimation closer to the actual value function. This is because we are continually moving our estimate closer to a target that contains more data from the actual environment. In addition to using <script type="math/tex">\delta_{TD}</script> to optimize our value network, it turns out that we can also use it to estimate advantage. Wait, what? How? Let’s bring back <script type="math/tex">q_{\pi}(s,a)</script>:</p>

<script type="math/tex; mode=display">q_{\pi}(s,a) = r + \gamma v_{\pi}(s^{\prime})</script>

<p>Recall what advantage is defined as:</p>

<script type="math/tex; mode=display">A = Q - V</script>

<p>Now let’s take a look at <script type="math/tex">\delta_{TD}</script> again:</p>

<script type="math/tex; mode=display">\delta_{TD} = \underbrace{r + \gamma v_{\pi}(s^{\prime})}_{q_{\pi}(s,a)} - v_{\pi}(s)</script>

<p>which means that <script type="math/tex">\delta_{TD} \approx A</script></p>

<h2 id="generalized-advantage-estimation">Generalized Advantage Estimation</h2>

<p>The <a href="https://arxiv.org/pdf/1506.02438.pdf">paper</a> we are referencing in this section was used for continuous control, but it can also be used for a discrete action space, like the one we are working with.</p>

<p>We will denote our advantage estimate as <script type="math/tex">\hat{A}_t</script>. Like any other estimate, <script type="math/tex">\hat{A}_t</script> is subject to bias (although it has low variance). To get an unbiased estimate, we need to get rid of the value estimate completely and sum all future rewards in an episode. This is known as the Monte Carlo return, and it has high variance. As with most things in machine learning, there is a tradeoff - this one is known as the bias-variance tradeoff in reinforcement learning. Generalized Advantage Estimation (GAE) is a great solution that significantly reduces variance while maintaining a tolerable level of bias. It is parametereized by <script type="math/tex">\gamma \in [0,1]</script> and <script type="math/tex">\lambda \in [0,1]</script>, where <script type="math/tex">\gamma</script> is the discount factor mentioned earlier in this blog, and <script type="math/tex">\lambda</script> is the decay parameter used to take an exponentially weighted average of k-step advantage estimators. It is analogous to <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.132.7760&amp;rep=rep1&amp;type=pdf">Sutton’s TD(<script type="math/tex">\lambda</script>)</a>.</p>

<p>Before we get into some of the math, I want to note that <script type="math/tex">\gamma</script> and <script type="math/tex">\lambda</script> serve different purposes. To determine the scale of the value function, we use <script type="math/tex">\gamma</script>. In other words, the value of <script type="math/tex">\gamma</script> determines how nearsighted (<script type="math/tex">\gamma</script> near 0) or farsighted (<script type="math/tex">\gamma</script> near 1) we want our agent to be in its value estimate. No matter how accurate our value function is, if <script type="math/tex">% <![CDATA[
\gamma < 1 %]]></script>, we introduce bias into the policy gradient estimate. On the other hand, <script type="math/tex">\lambda</script> is a decay factor and <script type="math/tex">% <![CDATA[
\lambda < 1 %]]></script> only introduces bias when the value function is inaccurate.</p>

<p>I’m going to spare you the details on the derivation of GAE because I feel like we’ve gone through enough math for one post. However, if you have any questions just let me know in the comments section below and I’ll explain it in-depth. As mentioned before, GAE is defined as the exponentially weighted average of k-step advantage estimators. The equation is shown below:</p>

<script type="math/tex; mode=display">\hat{A}^{GAE(\gamma,\lambda)}_t = \sum^{\infty}_{l=0}(\gamma \lambda)^l\delta_{t+l}</script>

<p>If you understand the equation above, then you might find this next part pretty cool, otherwise you can just skip over it. There are two special cases of the formula above, when <script type="math/tex">\lambda=0</script> and <script type="math/tex">\lambda=1</script>:</p>

<script type="math/tex; mode=display">GAE(\gamma,0): \hat{A}_t := \delta_t = r_t + \gamma V(S_{t+1}) - V(S_t)</script>

<script type="math/tex; mode=display">GAE(\gamma,1): \hat{A}_t := \sum^{\infty}_{l=0}\gamma\delta_{t+l} = \sum^{\infty}_{l=0}\gamma^lr_{t+l} - V(S_t)</script>

<p>When we have <script type="math/tex">% <![CDATA[
0 < \lambda < 1 %]]></script>, our GAE is making a compromise between bias and variance. From now on, our loss function for the policy gradient becomes:</p>

<script type="math/tex; mode=display">\mathcal{L} = \hat{A}^{GAE(\gamma,\lambda)} \times \log \pi(s,a)</script>

<p>Going forward, when you see <script type="math/tex">\hat{A}_t</script>, we are actually referring to <script type="math/tex">\hat{A}^{GAE(\gamma,\lambda)}_t</script>.</p>

<h2 id="proximal-policy-optimization">Proximal Policy Optimization</h2>

<p>We’re finally done catching up on all the background knowledge - time to learn about Proximal Policy Optimization (PPO)! This algorithm is from <a href="https://arxiv.org/pdf/1707.06347.pdf">OpenAI’s paper</a>, and I highly recommend checking it out to get a more in-depth understanding after reading my blog.</p>

<p>PPO takes inspiration from <a href="https://arxiv.org/pdf/1502.05477.pdf">Trust Region Policy Optimization</a> (TRPO), which maximizes a “surrogate” objective function:</p>

<script type="math/tex; mode=display">L^{CPI}(\theta) = \hat{\mathbb{E}}_t\big[r_t(\theta)\hat{A}_t\big]</script>

<p>where <script type="math/tex">r_t(\theta)</script> represents the probability ratio of our current policy versus our old policy:</p>

<script type="math/tex; mode=display">r_t(\theta) = \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{old}}(a_t \mid s_t)}</script>

<p>TRPO also has constraints that I’m not going to get into, but if you’re interested, I highly recommend reading the paper. While TRPO is quite impressive, it is complex and computationally expensive to run. As a result, OpenAI came up with a simpler, more general algorithm that has better sample complexity (empirically). The idea is to limit how much our policy can change during each round of updates by clipping <script type="math/tex">r_t(\theta)</script> between a range determined by <script type="math/tex">\epsilon</script>:</p>

<script type="math/tex; mode=display">L^{CLIP}(\theta) = \hat{\mathbb{E}}_t\big[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon)\hat{A}_t)\big]</script>

<p>The reason we do this is because conventional policy gradient methods are very sensitive to your choice of step size. If the step size is too small then the training progresses too slowly. If the step size is too large then your policy can overshoot the optimal policy during training, making it too noisy. By limiting how much our policy can change, we reduce the sensitivity to the step size.</p>

<p>You will notice in the image below (taken from the PPO paper) that there are certain values of <script type="math/tex">r_t(\theta)</script> where the gradient is 0. When the advantage is positive, the cutoff point is <script type="math/tex">1 + \epsilon</script>. When the advantage is negative, the cutoff point is <script type="math/tex">1 - \epsilon</script>. By taking the minimum of the clipped and unclipped objective, as demonstrated below, we are creating a lower bound on the unclipped objective. In other words, we ignore a change in <script type="math/tex">r_t(\theta)</script> when it makes the objective improve, which is why the lower bound is also known as the pessimistic bound.</p>

<p style="text-align: center;"><img src="http://localhost:4000/images/PPO-objective.png" alt="alt" /></p>

<p>The video shown below was created after only 10 minutes of training using PPO on a Macbook Pro. I plan on running the algorithm for longer and updating the video sometime this week (probably around the same time I release the code on github):</p>

<video controls="controls" allowfullscreen="true">
  <source src="/images/mario.avi" type="video/mp4" />
</video>

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>In this post, we covered a lot of reinforcement learning background and learned how PPO works. We see that using GAE with PPO is a clever way to deal with the credit assignment problem, while keeping bias in check. We also learned a little bit about convolutional neural networks as a way to deal with pixelated inputs. I hope you can take what you learned in this post and apply it to your favorite games!</p>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = http://localhost:4000;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = /mario-ppo; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://brandinho-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#policy-gradient" class="page__taxonomy-item" rel="tag">Policy Gradient</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#proximal-policy-optimization" class="page__taxonomy-item" rel="tag">Proximal Policy Optimization</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#reinforcement-learning" class="page__taxonomy-item" rel="tag">Reinforcement Learning</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-12-02T00:00:00-05:00">December 02, 2018</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Playing+Super+Mario+Bros+with+Proximal+Policy+Optimization%20http%3A%2F%2Flocalhost%3A4000%2Fmario-ppo%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fmario-ppo%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http%3A%2F%2Flocalhost%3A4000%2Fmario-ppo%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fab fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fmario-ppo%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/genetic-algorithm/" class="pagination--pager" title="Learning How to Run with Genetic Algorithms
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/genetic-algorithm/" rel="permalink">Learning How to Run with Genetic Algorithms
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  8 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Reinforcement Learning, Neural Networks, Genetic Algorithm
</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/bayesian-policy/" rel="permalink">Learning Probability Distributions in Bounded Action Spaces
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Reinforcement Learning, Neural Networks, Bayesian
</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Brandon Da Silva. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>








<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  </body>
</html>
